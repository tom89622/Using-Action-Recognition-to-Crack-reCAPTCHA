# Using-Action-Recognition-to-Crack-reCAPTCHA

## Summary
<!-- Summary -->
The goal of this project is to distinguish between machine and human beings when using reCAPTCHA with action. We implement the most powerful attacker and defense strategies to find out what degree of a cropped image will successfully distinguish machine and human beings. Also, to produce the cropped image for defense, we introduce the Grad-CAM technique to know where the reference of the attacker is and crop these places. Ultimately, we find the most effective method to defend the attacker, resulting in the attacker having 98%, 60%, and 65% accuracy for 3 categories.


## Technique usage
<!-- Technique usage -->
### Transfer-Learning
This project adopts Transfer Learning [1] and utilizes ResNet152 as the pretrained model for training. The underlying concept involves initially fixing the convolutional layers in the neural network architecture to enable the model to extract features from images, such as edges, corners, and contours. Once the model acquires the ability to obtain fundamental information about the underlying structure of images, the classifier is then retrained. Subsequently, with a lower learning rate, the entire model undergoes fine-tuning to adapt to new data, ultimately enhancing the overall learning performance of the model.

![image](https://github.com/tom89622/Using-Action-Recognition-to-Crack-reCAPTCHA/blob/0f26738022b270d495537fc10fbfd94c2d622227/image%20reference/Transfer%20Learning.png)


The advantage of using Transfer Learning is that it eliminates the need to gather a large amount of data for model training. This is particularly beneficial in the early stages of the project when there might not be an extensive dataset available, yet it allows the model to achieve a decent recognition rate. Additionally, there is no requirement to invest a significant amount of time in training the model from scratch. This aspect proves to be time-saving, especially in the later stages of the project when continuous optimization of the model is undertaken.


### Grad-CAM
We utilized the Grad-CAM [2] technique, based on the CAM [3] technology, to aid in understanding the regions of focus for the model. The working principle involves a concept similar to back-propagation, where the weights of pixels in various regions of the image are computed at each layer. These weights are then stacked on their corresponding pixels, ultimately generating a class activataion map (as shown in the diagram below). This technique provides insights into the specific areas the model emphasizes during its operations.

![image](https://github.com/tom89622/Using-Action-Recognition-to-Crack-reCAPTCHA/blob/0f26738022b270d495537fc10fbfd94c2d622227/image%20reference/Grad-CAM.png)

By examining the class activataion map, it becomes straightforward to assess the significance of a particular region in influencing the prediction results. In this project, we rely on these maps to evaluate whether the model training outcomes align with the expected attention areas. Additionally, the maps serve as a basis for determining the criteria for image cropping, which is crucial for subsequent training phases.


## DataSet example
<!-- Dataset -->

## Models
<!-- Models 
    process
    basic one
    gray
    level 1 
    level 2
    level 3 -->
### Training process
The objective of model training is to standardize the subject and emphasize the model's focus on human body movements. Therefore, experimental image cropping is employed following the workflow depicted in the following diagram. The aim is to train the model continuously in an attempt to push it closer to the recognition limit perceivable by humans. Furthermore, the goal is to identify the extent of image cropping that is sufficient to distinguish between machine and human recognition. This degree of cropping will be referred to as the 'sweet spot' of recognition in this project.

![image](https://github.com/tom89622/Using-Action-Recognition-to-Crack-reCAPTCHA/blob/0f26738022b270d495537fc10fbfd94c2d622227/image%20reference/Training%20diagram.png)

Therefore, in this phase, multiple versions will be developed following the workflow diagram until the 'sweet spot' is identified. For each version, the test images will reference the Grad-CAM heatmap generated by that version. This will help in summarizing the locations where the machine focuses its attention. These summaries will serve as the basis for cropping, with the aim of testing whether the model can still successfully recognize images after removing parts of the areas it concentrates on.


### Basic model
This version involves removing the basketball hoop factor from the training and validation sets. The objective is to achieve a generalization effect by including basketball occurrences across all categories, thereby eliminating specific features associated with the hoop. The expected outcome is for the model to rely on identifying the body parts of the subject.

![image](https://github.com/tom89622/Using-Action-Recognition-to-Crack-reCAPTCHA/blob/b136581dea22f73a1aa592c846f50b553d732d70/image%20reference/original%20image%20(Basic%20model).png)

![image](https://github.com/tom89622/Using-Action-Recognition-to-Crack-reCAPTCHA/blob/b136581dea22f73a1aa592c846f50b553d732d70/image%20reference/statistic%20(Basic%20model).png)

![image](https://github.com/tom89622/Using-Action-Recognition-to-Crack-reCAPTCHA/blob/f53b65cc55373909dbf7022fc0a22e9e8226b953/image%20reference/Grad-CAM%20image%20(Basic%20model).png)

Test results after training:

> **Dribbling: 100%,  Dunking: 85.3%, Shooting: 95.7%**

The following are the features summarized based on the Grad-CAM Picture:

* Dribbling: both sides of the head to the shoulders, ball
* Dunking: wrist, lower edge of the ball, court + lighting
* Shooting: arm, elbow, ball

From the results, it is known that the goal of focusing on the main character has been achieved, and the model no longer pays attention to the basket area,
Therefore, this version is chosen as the beginning of the flow chart. The next versions will continue to test the pictures after cropping,
If the accuracy is poor, then add them to the training set and retrain.


### Gray

### Level 1

### level 2

### level 3
<!-- conclusion -->
